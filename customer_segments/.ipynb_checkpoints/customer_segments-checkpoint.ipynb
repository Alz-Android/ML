{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Unsupervised Learning\n",
    "## Project: Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in *monetary units*) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n",
    "\n",
    "The dataset for this project can be found on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). For the purposes of this project, the features `'Channel'` and `'Region'` will be excluded in the analysis â€” with focus instead on the six product categories recorded for customers.\n",
    "\n",
    "Run the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicatessen']\n",
      "112151\n",
      "73498\n",
      "92780\n",
      "60869\n",
      "40827\n",
      "47943\n",
      "Wholesale customers dataset has 440 samples with 6 features each.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the wholesale customers dataset\n",
    "try:\n",
    "    data = pd.read_csv(\"customers.csv\")\n",
    "    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "\n",
    "    print list(data)\n",
    "    \n",
    "#    print data.Milk.max(axis=0)\n",
    "    \n",
    "    for feature in list(data):\n",
    "        print data[feature].max(axis=0)\n",
    "    \n",
    "    print \"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape)\n",
    "except:\n",
    "    print \"Dataset could not be loaded. Is the dataset missing?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "In this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project.\n",
    "\n",
    "Run the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: **'Fresh'**, **'Milk'**, **'Grocery'**, **'Frozen'**, **'Detergents_Paper'**, and **'Delicatessen'**. Consider what each category represents in terms of products you could purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicatessen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12000.297727</td>\n",
       "      <td>5796.265909</td>\n",
       "      <td>7951.277273</td>\n",
       "      <td>3071.931818</td>\n",
       "      <td>2881.493182</td>\n",
       "      <td>1524.870455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12647.328865</td>\n",
       "      <td>7380.377175</td>\n",
       "      <td>9503.162829</td>\n",
       "      <td>4854.673333</td>\n",
       "      <td>4767.854448</td>\n",
       "      <td>2820.105937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3127.750000</td>\n",
       "      <td>1533.000000</td>\n",
       "      <td>2153.000000</td>\n",
       "      <td>742.250000</td>\n",
       "      <td>256.750000</td>\n",
       "      <td>408.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8504.000000</td>\n",
       "      <td>3627.000000</td>\n",
       "      <td>4755.500000</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>816.500000</td>\n",
       "      <td>965.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16933.750000</td>\n",
       "      <td>7190.250000</td>\n",
       "      <td>10655.750000</td>\n",
       "      <td>3554.250000</td>\n",
       "      <td>3922.000000</td>\n",
       "      <td>1820.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>112151.000000</td>\n",
       "      <td>73498.000000</td>\n",
       "      <td>92780.000000</td>\n",
       "      <td>60869.000000</td>\n",
       "      <td>40827.000000</td>\n",
       "      <td>47943.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Fresh          Milk       Grocery        Frozen  \\\n",
       "count     440.000000    440.000000    440.000000    440.000000   \n",
       "mean    12000.297727   5796.265909   7951.277273   3071.931818   \n",
       "std     12647.328865   7380.377175   9503.162829   4854.673333   \n",
       "min         3.000000     55.000000      3.000000     25.000000   \n",
       "25%      3127.750000   1533.000000   2153.000000    742.250000   \n",
       "50%      8504.000000   3627.000000   4755.500000   1526.000000   \n",
       "75%     16933.750000   7190.250000  10655.750000   3554.250000   \n",
       "max    112151.000000  73498.000000  92780.000000  60869.000000   \n",
       "\n",
       "       Detergents_Paper  Delicatessen  \n",
       "count        440.000000    440.000000  \n",
       "mean        2881.493182   1524.870455  \n",
       "std         4767.854448   2820.105937  \n",
       "min            3.000000      3.000000  \n",
       "25%          256.750000    408.250000  \n",
       "50%          816.500000    965.500000  \n",
       "75%         3922.000000   1820.250000  \n",
       "max        40827.000000  47943.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a description of the dataset\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Selecting Samples\n",
    "To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add **three** indices of your choice to the `indices` list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "Chosen samples of wholesale customers dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicatessen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112151</td>\n",
       "      <td>29627</td>\n",
       "      <td>18148</td>\n",
       "      <td>16745</td>\n",
       "      <td>4948</td>\n",
       "      <td>8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
       "0    7057   9810     9568    1762              3293          1776\n",
       "1    6353   8808     7684    2405              3516          7844\n",
       "2  112151  29627    18148   16745              4948          8550\n",
       "3   22615   5410     7198    3915              1777          5185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaaf95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Select three indices of your choice you wish to sample from the dataset\n",
    "# min / max has outliers\n",
    "# use median cuz it's actual values in data\n",
    "# look at the hystogram for each feature\n",
    "\n",
    "#for feature in list(data):\n",
    "print data.index[data[feature]== int(data['Fresh'].max())]\n",
    "\n",
    "#print data.index[8504]\n",
    "\n",
    "indices = [1,2,181,4]\n",
    "\n",
    "data.hist()\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\n",
    "print \"Chosen samples of wholesale customers dataset:\"\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index[data['Fresh']== data['Fresh'].median()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8504.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['Fresh'].max()\n",
    "print(data['Fresh'].median())\n",
    "data.index[data['Fresh']== 8504.0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
      "0     12669   9656     7561     214              2674          1338\n",
      "1      7057   9810     9568    1762              3293          1776\n",
      "2      6353   8808     7684    2405              3516          7844\n",
      "3     13265   1196     4221    6404               507          1788\n",
      "4     22615   5410     7198    3915              1777          5185\n",
      "5      9413   8259     5126     666              1795          1451\n",
      "6     12126   3199     6975     480              3140           545\n",
      "7      7579   4956     9426    1669              3321          2566\n",
      "8      5963   3648     6192     425              1716           750\n",
      "9      6006  11093    18881    1159              7425          2098\n",
      "10     3366   5403    12974    4400              5977          1744\n",
      "11    13146   1124     4523    1420               549           497\n",
      "12    31714  12319    11757     287              3881          2931\n",
      "13    21217   6208    14982    3095              6707           602\n",
      "14    24653   9465    12091     294              5058          2168\n",
      "15    10253   1114     3821     397               964           412\n",
      "16     1020   8816    12121     134              4508          1080\n",
      "17     5876   6157     2933     839               370          4478\n",
      "18    18601   6327    10099    2205              2767          3181\n",
      "19     7780   2495     9464     669              2518           501\n",
      "20    17546   4519     4602    1066              2259          2124\n",
      "21     5567    871     2010    3383               375           569\n",
      "22    31276   1917     4469    9408              2381          4334\n",
      "23    26373  36423    22019    5154              4337         16523\n",
      "24    22647   9776    13792    2915              4482          5778\n",
      "25    16165   4230     7595     201              4003            57\n",
      "26     9898    961     2861    3151               242           833\n",
      "27    14276    803     3045     485               100           518\n",
      "28     4113  20484    25957    1158              8604          5206\n",
      "29    43088   2100     2609    1200              1107           823\n",
      "30    18815   3610    11107    1148              2134          2963\n",
      "31     2612   4339     3133    2088               820           985\n",
      "32    21632   1318     2886     266               918           405\n",
      "33    29729   4786     7326    6130               361          1083\n",
      "34     1502   1979     2262     425               483           395\n",
      "35      688   5491    11091     833              4239           436\n",
      "36    29955   4362     5428    1729               862          4626\n",
      "37    15168  10556    12477    1920              6506           714\n",
      "38     4591  15729    16709      33              6956           433\n",
      "39    56159    555      902   10002               212          2916\n",
      "40    24025   4332     4757    9510              1145          5864\n",
      "41    19176   3065     5956    2033              2575          2802\n",
      "42    10850   7555    14961     188              6899            46\n",
      "43      630  11095    23998     787              9529            72\n",
      "44     9670   7027    10471     541              4618            65\n",
      "45     5181  22044    21531    1740              7353          4985\n",
      "46     3103  14069    21955    1668              6792          1452\n",
      "47    44466  54259    55571    7782             24171          6465\n",
      "48    11519   6152    10868     584              5121          1476\n",
      "49     4967  21412    28921    1798             13583          1163\n",
      "50     6269   1095     1980    3860               609          2162\n",
      "51     3347   4051     6996     239              1538           301\n",
      "52    40721   3916     5876     532              2587          1278\n",
      "53      491  10473    11532     744              5611           224\n",
      "54    27329   1449     1947    2436               204          1333\n",
      "55     5264   3683     5005    1057              2024          1130\n",
      "56     4098  29892    26866    2616             17740          1340\n",
      "57     5417   9933    10487      38              7572          1282\n",
      "58    13779   1970     1648     596               227           436\n",
      "59     6137   5360     8040     129              3084          1603\n",
      "60     8590   3045     7854      96              4095           225\n",
      "61    35942  38369    59598    3254             26701          2017\n",
      "62     7823   6245     6544    4154              4074           964\n",
      "63     9396  11601    15775    2896              7677          1295\n",
      "64     4760   1227     3250    3724              1247          1145\n",
      "65       85  20959    45828      36             24231          1423\n",
      "66        9   1534     7417     175              3468            27\n",
      "67    19913   6759    13462    1256              5141           834\n",
      "68     2446   7260     3993    5870               788          3095\n",
      "69     8352   2820     1293     779               656           144\n",
      "70    16705   2037     3202   10643               116          1365\n",
      "71    18291   1266    21042    5373              4173         14472\n",
      "72     4420   5139     2661    8872              1321           181\n",
      "73    19899   5332     8713    8132               764           648\n",
      "74     8190   6343     9794    1285              1901          1780\n",
      "75    20398   1137        3    4407                 3           975\n",
      "76      717   3587     6532    7530               529           894\n",
      "77    12205  12697    28540     869             12034          1009\n",
      "78    10766   1175     2067    2096               301           167\n",
      "79     1640   3259     3655     868              1202          1653\n",
      "80     7005    829     3009     430               610           529\n",
      "81      219   9540    14403     283              7818           156\n",
      "82    10362   9232    11009     737              3537          2342\n",
      "83    20874   1563     1783    2320               550           772\n",
      "84    11867   3327     4814    1178              3837           120\n",
      "85    16117  46197    92780    1026             40827          2944\n",
      "86    22925  73498    32114     987             20070           903\n",
      "87    43265   5025     8117    6312              1579         14351\n",
      "88     7864    542     4042    9735               165            46\n",
      "89    24904   3836     5330    3443               454          3178\n",
      "90    11405    596     1638    3347                69           360\n",
      "91    12754   2762     2530    8693               627          1117\n",
      "92     9198  27472    32034    3232             18906          5130\n",
      "93    11314   3090     2062   35009                71          2698\n",
      "94     5626  12220    11323     206              5038           244\n",
      "95        3   2920     6252     440               223           709\n",
      "96       23   2616     8118     145              3874           217\n",
      "97      403    254      610     774                54            63\n",
      "98      503    112      778     895                56           132\n",
      "99     9658   2182     1909    5639               215           323\n",
      "100   11594   7779    12144    3252              8035          3029\n",
      "101    1420  10810    16267    1593              6766          1838\n",
      "102    2932   6459     7677    2561              4573          1386\n",
      "103   56082   3504     8906   18028              1480          2498\n",
      "104   14100   2132     3445    1336              1491           548\n",
      "105   15587   1014     3970     910               139          1378\n",
      "106    1454   6337    10704     133              6830          1831\n",
      "107    8797  10646    14886    2471              8969          1438\n",
      "108    1531   8397     6981     247              2505          1236\n",
      "109    1406  16729    28986     673               836             3\n",
      "110   11818   1648     1694    2276               169          1647\n",
      "111   12579  11114    17569     805              6457          1519\n",
      "112   19046   2770     2469    8853               483          2708\n",
      "113   14438   2295     1733    3220               585          1561\n",
      "114   18044   1080     2000    2555               118          1266\n",
      "115   11134    793     2988    2715               276           610\n",
      "116   11173   2521     3355    1517               310           222\n",
      "117    6990   3880     5380    1647               319          1160\n",
      "118   20049   1891     2362    5343               411           933\n",
      "119    8258   2344     2147    3896               266           635\n",
      "120   17160   1200     3412    2417               174          1136\n",
      "121    4020   3234     1498    2395               264           255\n",
      "122   12212    201      245    1991                25           860\n",
      "123   11170  10769     8814    2194              1976           143\n",
      "124   36050   1642     2961    4787               500          1621\n",
      "125   76237   3473     7102   16538               778           918\n",
      "126   19219   1840     1658    8195               349           483\n",
      "127   21465   7243    10685     880              2386          2749\n",
      "128     140   8847     3823     142              1062             3\n",
      "129   42312    926     1510    1718               410          1819\n",
      "130    7149   2428      699    6316               395           911\n",
      "131    2101    589      314     346                70           310\n",
      "132   14903   2032     2479     576               955           328\n",
      "133    9434   1042     1235     436               256           396\n",
      "134    7388   1882     2174     720                47           537\n",
      "135    6300   1289     2591    1170               199           326\n",
      "136    4625   8579     7030    4575              2447          1542\n",
      "137    3087   8080     8282     661               721            36\n",
      "138   13537   4257     5034     155               249          3271\n",
      "139    5387   4979     3343     825               637           929\n",
      "140   17623   4280     7305    2279               960          2616\n",
      "141   30379  13252     5189     321                51          1450\n",
      "142   37036   7152     8253    2995                20             3\n",
      "143   10405   1596     1096    8425               399           318\n",
      "144   18827   3677     1988     118               516           201\n",
      "145   22039   8384    34792      42             12591          4430\n",
      "146    7769   1936     2177     926                73           520\n",
      "147    9203   3373     2707    1286              1082           526\n",
      "148    5924    584      542    4052               283           434\n",
      "149   31812   1433     1651     800               113          1440\n",
      "150   16225   1825     1765     853               170          1067\n",
      "151    1289   3328     2022     531               255          1774\n",
      "152   18840   1371     3135    3001               352           184\n",
      "153    3463   9250     2368     779               302          1627\n",
      "154     622     55      137      75                 7             8\n",
      "155    1989  10690    19460     233             11577          2153\n",
      "156    3830   5291    14855     317              6694          3182\n",
      "157   17773   1366     2474    3378               811           418\n",
      "158    2861   6570     9618     930              4004          1682\n",
      "159     355   7704    14682     398              8077           303\n",
      "160    1725   3651    12822     824              4424          2157\n",
      "161   12434    540      283    1092                 3          2233\n",
      "162   15177   2024     3810    2665               232           610\n",
      "163    5531  15726    26870    2367             13726           446\n",
      "164    5224   7603     8584    2540              3674           238\n",
      "165   15615  12653    19858    4425              7108          2379\n",
      "166    4822   6721     9170     993              4973          3637\n",
      "167    2926   3195     3268     405              1680           693\n",
      "168    5809    735      803    1393                79           429\n",
      "169    5414    717     2155    2399                69           750\n",
      "170     260   8675    13430    1116              7015           323\n",
      "171     200  25862    19816     651              8773          6250\n",
      "172     955   5479     6536     333              2840           707\n",
      "173     514   7677    19805     937              9836           716\n",
      "174     286   1208     5241    2515               153          1442\n",
      "175    2343   7845    11874      52              4196          1697\n",
      "176   45640   6958     6536    7368              1532           230\n",
      "177   12759   7330     4533    1752                20          2631\n",
      "178   11002   7075     4945    1152               120           395\n",
      "179    3157   4888     2500    4477               273          2165\n",
      "180   12356   6036     8887     402              1382          2794\n",
      "181  112151  29627    18148   16745              4948          8550\n",
      "182     694   8533    10518     443              6907           156\n",
      "183   36847  43950    20170   36534               239         47943\n",
      "184     327    918     4710      74               334            11\n",
      "185    8170   6448     1139    2181                58           247\n",
      "186    3009    521      854    3470               949           727\n",
      "187    2438   8002     9819    6269              3459             3\n",
      "188    8040   7639    11687    2758              6839           404\n",
      "189     834  11577    11522     275              4027          1856\n",
      "190   16936   6250     1981    7332               118            64\n",
      "191   13624    295     1381     890                43            84\n",
      "192    5509   1461     2251     547               187           409\n",
      "193     180   3485    20292     959              5618           666\n",
      "194    7107   1012     2974     806               355          1142\n",
      "195   17023   5139     5230    7888               330          1755\n",
      "196   30624   7209     4897   18711               763          2876\n",
      "197    2427   7097    10391    1127              4314          1468\n",
      "198   11686   2154     6824    3527               592           697\n",
      "199    9670   2280     2112     520               402           347\n",
      "200    3067  13240    23127    3941              9959           731\n",
      "201    4484  14399    24708    3549             14235          1681\n",
      "202   25203  11487     9490    5065               284          6854\n",
      "203     583    685     2216     469               954            18\n",
      "204    1956    891     5226    1383                 5          1328\n",
      "205    1107  11711    23596     955              9265           710\n",
      "206    6373    780      950     878               288           285\n",
      "207    2541   4737     6089    2946              5316           120\n",
      "208    1537   3748     5838    1859              3381           806\n",
      "209    5550  12729    16767     864             12420           797\n",
      "210   18567   1895     1393    1801               244          2100\n",
      "211   12119  28326    39694    4736             19410          2870\n",
      "212    7291   1012     2062    1291               240          1775\n",
      "213    3317   6602     6861    1329              3961          1215\n",
      "214    2362   6551    11364     913              5957           791\n",
      "215    2806  10765    15538    1374              5828          2388\n",
      "216    2532  16599    36486     179             13308           674\n",
      "217   18044   1475     2046    2532               130          1158\n",
      "218      18   7504    15205    1285              4797          6372\n",
      "219    4155    367     1390    2306                86           130\n",
      "220   14755    899     1382    1765                56           749\n",
      "221    5396   7503    10646      91              4167           239\n",
      "222    5041   1115     2856    7496               256           375\n",
      "223    2790   2527     5265    5612               788          1360\n",
      "224    7274    659     1499     784                70           659\n",
      "225   12680   3243     4157     660               761           786\n",
      "226   20782   5921     9212    1759              2568          1553\n",
      "227    4042   2204     1563    2286               263           689\n",
      "228    1869    577      572     950              4762           203\n",
      "229    8656   2746     2501    6845               694           980\n",
      "230   11072   5989     5615    8321               955          2137\n",
      "231    2344  10678     3828    1439              1566           490\n",
      "232   25962   1780     3838     638               284           834\n",
      "233     964   4984     3316     937               409             7\n",
      "234   15603   2703     3833    4260               325          2563\n",
      "235    1838   6380     2824    1218              1216           295\n",
      "236    8635    820     3047    2312               415           225\n",
      "237   18692   3838      593    4634                28          1215\n",
      "238    7363    475      585    1112                72           216\n",
      "239   47493   2567     3779    5243               828          2253\n",
      "240   22096   3575     7041   11422               343          2564\n",
      "241   24929   1801     2475    2216               412          1047\n",
      "242   18226    659     2914    3752               586           578\n",
      "243   11210   3576     5119     561              1682          2398\n",
      "244    6202   7775    10817    1183              3143          1970\n",
      "245    3062   6154    13916     230              8933          2784\n",
      "246    8885   2428     1777    1777               430           610\n",
      "247   13569    346      489    2077                44           659\n",
      "248   15671   5279     2406     559               562           572\n",
      "249    8040   3795     2070    6340               918           291\n",
      "250    3191   1993     1799    1730               234           710\n",
      "251    6134  23133    33586    6746             18594          5121\n",
      "252    6623   1860     4740    7683               205          1693\n",
      "253   29526   7961    16966     432               363          1391\n",
      "254   10379  17972     4748    4686              1547          3265\n",
      "255   31614    489     1495    3242               111           615\n",
      "256   11092   5008     5249     453               392           373\n",
      "257    8475   1931     1883    5004              3593           987\n",
      "258   56083   4563     2124    6422               730          3321\n",
      "259   53205   4959     7336    3012               967           818\n",
      "260    9193   4885     2157     327               780           548\n",
      "261    7858   1110     1094    6818                49           287\n",
      "262   23257   1372     1677     982               429           655\n",
      "263    2153   1115     6684    4324              2894           411\n",
      "264    1073   9679    15445      61              5980          1265\n",
      "265    5909  23527    13699   10155               830          3636\n",
      "266     572   9763    22182    2221              4882          2563\n",
      "267   20893   1222     2576    3975               737          3628\n",
      "268   11908   8053    19847    1069              6374           698\n",
      "269   15218    258     1138    2516               333           204\n",
      "270    4720   1032      975    5500               197            56\n",
      "271    2083   5007     1563    1120               147          1550\n",
      "272     514   8323     6869     529                93          1040\n",
      "273   36817   3045     1493    4802               210          1824\n",
      "274     894   1703     1841     744               759          1153\n",
      "275     680   1610      223     862                96           379\n",
      "276   27901   3749     6964    4479               603          2503\n",
      "277    9061    829      683   16919               621           139\n",
      "278   11693   2317     2543    5845               274          1409\n",
      "279   17360   6200     9694    1293              3620          1721\n",
      "280    3366   2884     2431     977               167          1104\n",
      "281   12238   7108     6235    1093              2328          2079\n",
      "282   49063   3965     4252    5970              1041          1404\n",
      "283   25767   3613     2013   10303               314          1384\n",
      "284   68951   4411    12609    8692               751          2406\n",
      "285   40254    640     3600    1042               436            18\n",
      "286    7149   2247     1242    1619              1226           128\n",
      "287   15354   2102     2828    8366               386          1027\n",
      "288   16260    594     1296     848               445           258\n",
      "289   42786    286      471    1388                32            22\n",
      "290    2708   2160     2642     502               965          1522\n",
      "291    6022   3354     3261    2507               212           686\n",
      "292    2838   3086     4329    3838               825          1060\n",
      "293    3996  11103    12469     902              5952           741\n",
      "294   21273   2013     6550     909               811          1854\n",
      "295    7588   1897     5234     417              2208           254\n",
      "296   19087   1304     3643    3045               710           898\n",
      "297    8090   3199     6986    1455              3712           531\n",
      "298    6758   4560     9965     934              4538          1037\n",
      "299     444    879     2060     264               290           259\n",
      "300   16448   6243     6360     824              2662          2005\n",
      "301    5283  13316    20399    1809              8752           172\n",
      "302    2886   5302     9785     364              6236           555\n",
      "303    2599   3688    13829     492             10069            59\n",
      "304     161   7460    24773     617             11783          2410\n",
      "305     243  12939     8852     799              3909           211\n",
      "306    6468  12867    21570    1840              7558          1543\n",
      "307   17327   2374     2842    1149               351           925\n",
      "308    6987   1020     3007     416               257           656\n",
      "309     918  20655    13567    1465              6846           806\n",
      "310    7034   1492     2405   12569               299          1117\n",
      "311   29635   2335     8280    3046               371           117\n",
      "312    2137   3737    19172    1274             17120           142\n",
      "313    9784    925     2405    4447               183           297\n",
      "314   10617   1795     7647    1483               857          1233\n",
      "315    1479  14982    11924     662              3891          3508\n",
      "316    7127   1375     2201    2679                83          1059\n",
      "317    1182   3088     6114     978               821          1637\n",
      "318   11800   2713     3558    2121               706            51\n",
      "319    9759  25071    17645    1128             12408          1625\n",
      "320    1774   3696     2280     514               275           834\n",
      "321    9155   1897     5167    2714               228          1113\n",
      "322   15881    713     3315    3703              1470           229\n",
      "323   13360    944    11593     915              1679           573\n",
      "324   25977   3587     2464    2369               140          1092\n",
      "325   32717  16784    13626   60869              1272          5609\n",
      "326    4414   1610     1431    3498               387           834\n",
      "327     542    899     1664     414                88           522\n",
      "328   16933   2209     3389    7849               210          1534\n",
      "329    5113   1486     4583    5127               492           739\n",
      "330    9790   1786     5109    3570               182          1043\n",
      "331   11223  14881    26839    1234              9606          1102\n",
      "332   22321   3216     1447    2208               178          2602\n",
      "333    8565   4980    67298     131             38102          1215\n",
      "334   16823    928     2743   11559               332          3486\n",
      "335   27082   6817    10790    1365              4111          2139\n",
      "336   13970   1511     1330     650               146           778\n",
      "337    9351   1347     2611    8170               442           868\n",
      "338       3    333     7021   15601                15           550\n",
      "339    2617   1188     5332    9584               573          1942\n",
      "340     381   4025     9670     388              7271          1371\n",
      "341    2320   5763    11238     767              5162          2158\n",
      "342     255   5758     5923     349              4595          1328\n",
      "343    1689   6964    26316    1456             15469            37\n",
      "344    3043   1172     1763    2234               217           379\n",
      "345    1198   2602     8335     402              3843           303\n",
      "346    2771   6939    15541    2693              6600          1115\n",
      "347   27380   7184    12311    2809              4621          1022\n",
      "348    3428   2380     2028    1341              1184           665\n",
      "349    5981  14641    20521    2005             12218           445\n",
      "350    3521   1099     1997    1796               173           995\n",
      "351    1210  10044    22294    1741             12638          3137\n",
      "352     608   1106     1533     830                90           195\n",
      "353     117   6264    21203     228              8682          1111\n",
      "354   14039   7393     2548    6386              1333          2341\n",
      "355     190    727     2012     245               184           127\n",
      "356   22686    134      218    3157                 9           548\n",
      "357      37   1275    22272     137              6747           110\n",
      "358     759  18664     1660    6114               536          4100\n",
      "359     796   5878     2109     340               232           776\n",
      "360   19746   2872     2006    2601               468           503\n",
      "361    4734    607      864    1206               159           405\n",
      "362    2121   1601     2453     560               179           712\n",
      "363    4627    997     4438     191              1335           314\n",
      "364    2615    873     1524    1103               514           468\n",
      "365    4692   6128     8025    1619              4515          3105\n",
      "366    9561   2217     1664    1173               222           447\n",
      "367    3477    894      534    1457               252           342\n",
      "368   22335   1196     2406    2046               101           558\n",
      "369    6211    337      683    1089                41           296\n",
      "370   39679   3944     4955    1364               523          2235\n",
      "371   20105   1887     1939    8164               716           790\n",
      "372    3884   3801     1641     876               397          4829\n",
      "373   15076   6257     7398    1504              1916          3113\n",
      "374    6338   2256     1668    1492               311           686\n",
      "375    5841   1450     1162     597               476            70\n",
      "376    3136   8630    13586    5641              4666          1426\n",
      "377   38793   3154     2648    1034                96          1242\n",
      "378    3225   3294     1902     282                68          1114\n",
      "379    4048   5164    10391     130               813           179\n",
      "380   28257    944     2146    3881               600           270\n",
      "381   17770   4591     1617    9927               246           532\n",
      "382   34454   7435     8469    2540              1711          2893\n",
      "383    1821   1364     3450    4006               397           361\n",
      "384   10683  21858    15400    3635               282          5120\n",
      "385   11635    922     1614    2583               192          1068\n",
      "386    1206   3620     2857    1945               353           967\n",
      "387   20918   1916     1573    1960               231           961\n",
      "388    9785    848     1172    1677               200           406\n",
      "389    9385   1530     1422    3019               227           684\n",
      "390    3352   1181     1328    5502               311          1000\n",
      "391    2647   2761     2313     907                95          1827\n",
      "392     518   4180     3600     659               122           654\n",
      "393   23632   6730     3842    8620               385           819\n",
      "394   12377    865     3204    1398               149           452\n",
      "395    9602   1316     1263    2921               841           290\n",
      "396    4515  11991     9345    2644              3378          2213\n",
      "397   11535   1666     1428    6838                64           743\n",
      "398   11442   1032      582    5390                74           247\n",
      "399    9612    577      935    1601               469           375\n",
      "400    4446    906     1238    3576               153          1014\n",
      "401   27167   2801     2128   13223                92          1902\n",
      "402   26539   4753     5091     220                10           340\n",
      "403   25606  11006     4604     127               632           288\n",
      "404   18073   4613     3444    4324               914           715\n",
      "405    6884   1046     1167    2069               593           378\n",
      "406   25066   5010     5026    9806              1092           960\n",
      "407    7362  12844    18683    2854              7883           553\n",
      "408    8257   3880     6407    1646              2730           344\n",
      "409    8708   3634     6100    2349              2123          5137\n",
      "410    6633   2096     4563    1389              1860          1892\n",
      "411    2126   3289     3281    1535               235          4365\n",
      "412      97   3605    12400      98              2970            62\n",
      "413    4983   4859     6633   17866               912          2435\n",
      "414    5969   1990     3417    5679              1135           290\n",
      "415    7842   6046     8552    1691              3540          1874\n",
      "416    4389  10940    10908     848              6728           993\n",
      "417    5065   5499    11055     364              3485          1063\n",
      "418     660   8494    18622     133              6740           776\n",
      "419    8861   3783     2223     633              1580          1521\n",
      "420    4456   5266    13227      25              6818          1393\n",
      "421   17063   4847     9053    1031              3415          1784\n",
      "422   26400   1377     4172     830               948          1218\n",
      "423   17565   3686     4657    1059              1803           668\n",
      "424   16980   2884    12232     874              3213           249\n",
      "425   11243   2408     2593   15348               108          1886\n",
      "426   13134   9347    14316    3141              5079          1894\n",
      "427   31012  16687     5429   15082               439          1163\n",
      "428    3047   5970     4910    2198               850           317\n",
      "429    8607   1750     3580      47                84          2501\n",
      "430    3097   4230    16483     575               241          2080\n",
      "431    8533   5506     5160   13486              1377          1498\n",
      "432   21117   1162     4754     269              1328           395\n",
      "433    1982   3218     1493    1541               356          1449\n",
      "434   16731   3922     7994     688              2371           838\n",
      "435   29703  12051    16027   13135               182          2204\n",
      "436   39228   1431      764    4510                93          2346\n",
      "437   14531  15488    30243     437             14841          1867\n",
      "438   10290   1981     2232    1038               168          2125\n",
      "439    2787   1698     2510      65               477            52\n"
     ]
    }
   ],
   "source": [
    "print(data.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh               8504.0\n",
      "Milk                3627.0\n",
      "Grocery             4755.5\n",
      "Frozen              1526.0\n",
      "Detergents_Paper     816.5\n",
      "Delicatessen         965.5\n",
      "dtype: float64\n",
      "Fresh                8533\n",
      "Milk                 5506\n",
      "Grocery              5160\n",
      "Frozen              13486\n",
      "Detergents_Paper     1377\n",
      "Delicatessen         1498\n",
      "Name: 431, dtype: int64\n",
      "Fresh               8475\n",
      "Milk                1931\n",
      "Grocery             1883\n",
      "Frozen              5004\n",
      "Detergents_Paper    3593\n",
      "Delicatessen         987\n",
      "Name: 257, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data.median()\n",
    "\n",
    "data.loc[data['Fresh']==data['Fresh'].median()]\n",
    "\n",
    "data.sort_values(['Fresh'], inplace = True)\n",
    "print data[data['Fresh'] > data['Fresh'].median()].iloc[0]\n",
    "print data[data['Fresh'] < data['Fresh'].median()].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "[8533L, 5506L, 5160L, 13486L, 1377L, 1498L]\n"
     ]
    }
   ],
   "source": [
    "data.sort_values(['Fresh'], inplace = True)\n",
    "# print data[data['Fresh'] > data['Fresh'].median()].iloc[0].iloc[5]\n",
    "# print data[data['Fresh'] < data['Fresh'].median()].iloc[-1]\n",
    "\n",
    "print(type(data[data['Fresh'] > data['Fresh'].median()].iloc[0]))\n",
    "print data[data['Fresh'] > data['Fresh'].median()].iloc[0].tolist().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-af83914e0fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fresh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Al\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by)\u001b[0m\n\u001b[0;32m   3326\u001b[0m                                     inplace=inplace)\n\u001b[0;32m   3327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3328\u001b[1;33m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3329\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Al\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_axis_number\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ALIASES\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_NAMES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print data.sort_index(['Fresh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Consider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers.  \n",
    "\n",
    "* What kind of establishment (customer) could each of the three samples you've chosen represent?\n",
    "\n",
    "**Hint:** Examples of establishments include places like markets, cafes, delis, wholesale retailers, among many others. Avoid using names for establishments, such as saying *\"McDonalds\"* when describing a sample customer as a restaurant. You can use the mean values for reference to compare your samples with. The mean values are as follows:\n",
    "\n",
    "* Fresh: 12000.2977\n",
    "* Milk: 5796.2\n",
    "* Grocery: 3071.9\n",
    "* Detergents_paper: 2881.4\n",
    "* Delicatessen: 1524.8\n",
    "\n",
    "Knowing this, how do your samples compare? Does that help in driving your insight into what kind of establishments they might be? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Feature Relevance\n",
    "One interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign `new_data` a copy of the data by removing a feature of your choice using the `DataFrame.drop` function.\n",
    " - Use `sklearn.cross_validation.train_test_split` to split the dataset into training and testing sets.\n",
    "   - Use the removed feature as your target label. Set a `test_size` of `0.25` and set a `random_state`.\n",
    " - Import a decision tree regressor, set a `random_state`, and fit the learner to the training data.\n",
    " - Report the prediction score of the testing set using the regressor's `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "new_data = None\n",
    "\n",
    "# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\n",
    "# Set a random state.\n",
    "X_train, X_test, y_train, y_test = (None, None, None, None)\n",
    "\n",
    "# TODO: Create a decision tree regressor and fit it to the training set\n",
    "regressor = None\n",
    "\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "* Which feature did you attempt to predict? \n",
    "* What was the reported prediction score? \n",
    "* Is this feature necessary for identifying customers' spending habits?\n",
    "\n",
    "**Hint:** The coefficient of determination, `R^2`, is scored between 0 and 1, with 1 being a perfect fit. A negative `R^2` implies the model fails to fit the data. If you get a low score for a particular feature, that lends us to beleive that that feature point is hard to predict using the other features, thereby making it an important feature to consider when considering relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Distributions\n",
    "To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produce a scatter matrix for each pair of features in the data\n",
    "pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "* Using the scatter matrix as a reference, discuss the distribution of the dataset, specifically talk about the normality, outliers, large number of data points near 0 among others. If you need to sepearate out some of the plots individually to further accentuate your point, you may do so as well.\n",
    "* Are there any pairs of features which exhibit some degree of correlation? \n",
    "* Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? \n",
    "* How is the data for those features distributed?\n",
    "\n",
    "**Hint:** Is the data normally distributed? Where do most of the data points lie? You can use [corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) to get the feature correlations and then visualize them using a [heatmap](http://seaborn.pydata.org/generated/seaborn.heatmap.html)(the data that would be fed into the heatmap would be the correlation values, for eg: `data.corr()`) to gain further insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Feature Scaling\n",
    "If data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics) to apply a non-linear scaling â€” particularly for financial data. One way to achieve this scaling is by using a [Box-Cox test](http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html), which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign a copy of the data to `log_data` after applying logarithmic scaling. Use the `np.log` function for this.\n",
    " - Assign a copy of the sample data to `log_samples` after applying logarithmic scaling. Again, use `np.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Scale the data using the natural logarithm\n",
    "log_data = None\n",
    "\n",
    "# TODO: Scale the sample data using the natural logarithm\n",
    "log_samples = None\n",
    "\n",
    "# Produce a scatter matrix for each pair of newly-transformed features\n",
    "pd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "After applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).\n",
    "\n",
    "Run the code below to see how the sample data has changed after having the natural logarithm applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the log-transformed sample data\n",
    "display(log_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Outlier Detection\n",
    "Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign the value of the 25th percentile for the given feature to `Q1`. Use `np.percentile` for this.\n",
    " - Assign the value of the 75th percentile for the given feature to `Q3`. Again, use `np.percentile`.\n",
    " - Assign the calculation of an outlier step for the given feature to `step`.\n",
    " - Optionally remove data points from the dataset by adding indices to the `outliers` list.\n",
    "\n",
    "**NOTE:** If you choose to remove any outliers, ensure that the sample data does not contain any of these points!  \n",
    "Once you have performed this implementation, the dataset will be stored in the variable `good_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each feature find the data points with extreme high or low values\n",
    "for feature in log_data.keys():\n",
    "    \n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = None\n",
    "    \n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = None\n",
    "    \n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = None\n",
    "    \n",
    "    # Display the outliers\n",
    "    print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n",
    "    \n",
    "# OPTIONAL: Select the indices for data points you wish to remove\n",
    "outliers  = []\n",
    "\n",
    "# Remove the outliers, if any were specified\n",
    "good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "* Are there any data points considered outliers for more than one feature based on the definition above? \n",
    "* Should these data points be removed from the dataset? \n",
    "* If any data points were added to the `outliers` list to be removed, explain why.\n",
    "\n",
    "** Hint: ** If you have datapoints that are outliers in multiple categories think about why that may be and if they warrant removal. Also note how k-means is affected by outliers and whether or not this plays a factor in your analysis of whether or not to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "In this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: PCA\n",
    "\n",
    "Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the *explained variance ratio* of each dimension â€” how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Import `sklearn.decomposition.PCA` and assign the results of fitting PCA in six dimensions with `good_data` to `pca`.\n",
    " - Apply a PCA transformation of `log_samples` using `pca.transform`, and assign the results to `pca_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "pca = None\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = None\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(good_data, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "* How much variance in the data is explained* **in total** *by the first and second principal component? \n",
    "* How much variance in the data is explained by the first four principal components? \n",
    "* Using the visualization provided above, talk about each dimension and the cumulative variance explained by each, stressing upon which features are well represented by each dimension(both in terms of positive and negative variance explained). Discuss what the first four dimensions best represent in terms of customer spending.\n",
    "\n",
    "**Hint:** A positive increase in a specific dimension corresponds with an *increase* of the *positive-weighted* features and a *decrease* of the *negative-weighted* features. The rate of increase or decrease is based on the individual feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display sample log-data after having a PCA transformation applied\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Dimensionality Reduction\n",
    "When using principal component analysis, one of the main goals is to reduce the dimensionality of the data â€” in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the *cumulative explained variance ratio* is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign the results of fitting PCA in two dimensions with `good_data` to `pca`.\n",
    " - Apply a PCA transformation of `good_data` using `pca.transform`, and assign the results to `reduced_data`.\n",
    " - Apply a PCA transformation of `log_samples` using `pca.transform`, and assign the results to `pca_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = None\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = None\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = None\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Biplot\n",
    "A biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case `Dimension 1` and `Dimension 2`). In addition, the biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features.\n",
    "\n",
    "Run the code cell below to produce a biplot of the reduced-dimension data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a biplot\n",
    "vs.biplot(good_data, reduced_data, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Once we have the original feature projections (in red), it is easier to interpret the relative position of each data point in the scatterplot. For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on `'Milk'`, `'Grocery'` and `'Detergents_Paper'`, but not so much on the other product categories. \n",
    "\n",
    "From the biplot, which of the original features are most strongly correlated with the first component? What about those that are associated with the second component? Do these observations agree with the pca_results plot you obtained earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "* What are the advantages to using a K-Means clustering algorithm? \n",
    "* What are the advantages to using a Gaussian Mixture Model clustering algorithm? \n",
    "* Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why?\n",
    "\n",
    "** Hint: ** Think about the differences between hard clustering and soft clustering and which would be appropriate for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Creating Clusters\n",
    "Depending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known *a priori*, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data â€” if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's *silhouette coefficient*. The [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the *mean* silhouette coefficient provides for a simple scoring method of a given clustering.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Fit a clustering algorithm to the `reduced_data` and assign it to `clusterer`.\n",
    " - Predict the cluster for each data point in `reduced_data` using `clusterer.predict` and assign them to `preds`.\n",
    " - Find the cluster centers using the algorithm's respective attribute and assign them to `centers`.\n",
    " - Predict the cluster for each sample data point in `pca_samples` and assign them `sample_preds`.\n",
    " - Import `sklearn.metrics.silhouette_score` and calculate the silhouette score of `reduced_data` against `preds`.\n",
    "   - Assign the silhouette score to `score` and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply your clustering algorithm of choice to the reduced data \n",
    "clusterer = None\n",
    "\n",
    "# TODO: Predict the cluster for each data point\n",
    "preds = None\n",
    "\n",
    "# TODO: Find the cluster centers\n",
    "centers = None\n",
    "\n",
    "# TODO: Predict the cluster for each transformed sample data point\n",
    "sample_preds = None\n",
    "\n",
    "# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "* Report the silhouette score for several cluster numbers you tried. \n",
    "* Of these, which number of clusters has the best silhouette score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Visualization\n",
    "Once you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the results of the clustering from implementation\n",
    "vs.cluster_results(reduced_data, preds, centers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Data Recovery\n",
    "Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the *averages* of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to *the average customer of that segment*. Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Apply the inverse transform to `centers` using `pca.inverse_transform` and assign the new centers to `log_centers`.\n",
    " - Apply the inverse function of `np.log` to `log_centers` using `np.exp` and assign the true centers to `true_centers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Inverse transform the centers\n",
    "log_centers = None\n",
    "\n",
    "# TODO: Exponentiate the centers\n",
    "true_centers = None\n",
    "\n",
    "# Display the true centers\n",
    "segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\n",
    "true_centers.index = segments\n",
    "display(true_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "* Consider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project(specifically looking at the mean values for the various feature points). What set of establishments could each of the customer segments represent?\n",
    "\n",
    "**Hint:** A customer who is assigned to `'Cluster X'` should best identify with the establishments represented by the feature set of `'Segment X'`. Think about what each segment represents in terms their values for the feature points chosen. Reference these values with the mean values to get some perspective into what kind of establishment they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "* For each sample point, which customer segment from* **Question 8** *best represents it? \n",
    "* Are the predictions for each sample point consistent with this?*\n",
    "\n",
    "Run the code block below to find which cluster each sample point is predicted to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the predictions\n",
    "for i, pred in enumerate(sample_preds):\n",
    "    print \"Sample point\", i, \"predicted to be in Cluster\", pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, you will investigate ways that you can make use of the clustered data. First, you will consider how the different groups of customers, the ***customer segments***, may be affected differently by a specific delivery scheme. Next, you will consider how giving a label to each customer (which *segment* that customer belongs to) can provide for additional features about the customer data. Finally, you will compare the ***customer segments*** to a hidden variable present in the data, to see whether the clustering identified certain relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 10\n",
    "Companies will often run [A/B tests](https://en.wikipedia.org/wiki/A/B_testing) when making small changes to their products or services to determine whether making that change will affect its customers positively or negatively. The wholesale distributor is considering changing its delivery service from currently 5 days a week to 3 days a week. However, the distributor will only make this change in delivery service for customers that react positively. \n",
    "\n",
    "* How can the wholesale distributor use the customer segments to determine which customers, if any, would react positively to the change in delivery service?*\n",
    "\n",
    "**Hint:** Can we assume the change affects all customers equally? How can we determine which group of customers it affects the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Additional structure is derived from originally unlabeled data when using clustering techniques. Since each customer has a ***customer segment*** it best identifies with (depending on the clustering algorithm applied), we can consider *'customer segment'* as an **engineered feature** for the data. Assume the wholesale distributor recently acquired ten new customers and each provided estimates for anticipated annual spending of each product category. Knowing these estimates, the wholesale distributor wants to classify each new customer to a ***customer segment*** to determine the most appropriate delivery service.  \n",
    "* How can the wholesale distributor label the new customers using only their estimated product spending and the **customer segment** data?\n",
    "\n",
    "**Hint:** A supervised learner could be used to train on the original customers. What would be the target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Underlying Distributions\n",
    "\n",
    "At the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset.\n",
    "\n",
    "Run the code block below to see how each data point is labeled either `'HoReCa'` (Hotel/Restaurant/Cafe) or `'Retail'` the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the clustering results based on 'Channel' data\n",
    "vs.channel_results(reduced_data, outliers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "* How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? \n",
    "* Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? \n",
    "* Would you consider these classifications as consistent with your previous definition of the customer segments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
